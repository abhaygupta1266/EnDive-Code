{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30370,"status":"ok","timestamp":1736835373575,"user":{"displayName":"Jacob Cheung","userId":"11124663735002671005"},"user_tz":300},"id":"eCl5sMyHgCfb","outputId":"28742e33-3c36-4a0c-9678-d32fc1a1e105"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2795,"status":"ok","timestamp":1736836384204,"user":{"displayName":"Jacob Cheung","userId":"11124663735002671005"},"user_tz":300},"id":"3RTiPn1YhXLp","outputId":"0b3f612a-aa11-4c11-c444-aa34a262a50c"},"outputs":[],"source":["# NEW CODE\n","\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","\n","gpt_base = \"/content/drive/MyDrive/!!Multi-AAVENUE/BLEU Score Filtered Datasets/GPT 4o\"\n","multi_value_base = \"/content/drive/MyDrive/!!Multi-AAVENUE/BLEU Score Filtered Datasets/Multi-VALUE/\"\n","output_base = \"/content/drive/MyDrive/!!Multi-AAVENUE/Aligned Translations/\"\n","\n","datasets = {\n","    \"FOLIO(1000)\": {\"original_header\": \"Premises\", \"file_ext\": \"csv\"},\n","    \"GSM8K(1000)\": {\"original_header\": \"Original\", \"file_ext\": \"csv\"},\n","    \"HumanEVAL(164)\": {\"original_header\": \"Prompt\", \"file_ext\": \"csv\"},\n","    \"Logic Bench MCQ(480)\": {\"original_header\": \"Context\", \"dialect_header\": \"DIALECT (context)\", \"file_ext\": \"csv\"},\n","    \"Logic Bench YN(500)\": {\"original_header\": \"Context\", \"dialect_header\": \"DIALECT (context)\", \"file_ext\": \"csv\"},\n","    \"MBPP(374)\": {\"original_header\": \"Original\", \"file_ext\": \"csv\"},\n","    \"SVAMP(700)\": {\"original_header\": \"Original\", \"file_ext\": \"csv\"},\n","    \"BoolQ (1000)\": {\"original_header\": \"SAE Passage\", \"file_ext\": \"csv\"},\n","    \"COPA (500)\": {\"original_header\": \"Premise\", \"file_ext\": \"csv\"},\n","    \"MultiRC (1000)\": {\"original_header\": \"Paragraph\", \"file_ext\": \"csv\"},\n","    \"SST-2 (1000)\": {\"original_header\": \"Original Sentence\", \"file_ext\": \"csv\"},\n","    \"WSC (659)\": {\"original_header\": \"Original Paragraph\", \"file_ext\": \"csv\"}\n","}\n","\n","dialects = [\"AAVE\", \"IndE\", \"JamE\", \"CollSgE\", \"ChcE\"]\n","\n","def read_csv(file_path, dialect, original_header, dialect_header):\n","    try:\n","        df = pd.read_csv(file_path)\n","        if original_header not in df.columns:\n","            print(f\"Column '{original_header}' not found in {file_path}\")\n","            return pd.DataFrame()\n","        if dialect_header not in df.columns:\n","            print(f\"Column '{dialect_header}' not found in {file_path}\")\n","            return pd.DataFrame()\n","        return pd.DataFrame({\n","            'Original': df[original_header],\n","            'Translated': df[dialect_header]\n","        }).drop_duplicates(subset=['Original'], keep='first')\n","    except Exception as e:\n","        print(f\"Error reading file {file_path}: {str(e)}\")\n","        return pd.DataFrame()\n","\n","def process_dataset(dialect, dataset):\n","    is_glue = dataset in [\"BoolQ (1000)\", \"COPA (500)\", \"MultiRC (1000)\", \"SST-2 (1000)\", \"WSC (659)\"]\n","\n","    if is_glue:\n","        output_dir = os.path.join(output_base, dialect, \"GLUE + SuperGLUE\")\n","    else:\n","        output_dir = os.path.join(output_base, dialect)\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    if is_glue:\n","        gpt_path = os.path.join(gpt_base, dialect, \"GLUE + SuperGLUE\", dataset, f\"{dataset}_filtered_bleu_scores.csv\")\n","        mv_path = os.path.join(multi_value_base, dialect, \"GLUE + SuperGLUE\", dataset, f\"{dataset}_filtered_bleu_scores.csv\")\n","    else:\n","        gpt_path = os.path.join(gpt_base, dialect, dataset, f\"{dataset}_filtered_bleu_scores.csv\")\n","        mv_path = os.path.join(multi_value_base, dialect, dataset, f\"{dataset}_filtered_bleu_scores.csv\")\n","\n","    original_header = datasets[dataset]['original_header']\n","    dialect_header = datasets[dataset].get('dialect_header', f\"{dialect} ({original_header})\").replace('DIALECT', dialect)\n","    gpt_df = read_csv(gpt_path, dialect, original_header, dialect_header)\n","    mv_df = read_csv(mv_path, dialect, original_header, dialect_header)\n","\n","    if gpt_df.empty or mv_df.empty:\n","        print(f\"Skipping {dialect} - {dataset} due to empty DataFrame\")\n","        return\n","\n","    merged_df = pd.merge(\n","        gpt_df,\n","        mv_df,\n","        on='Original',\n","        how='inner',\n","        suffixes=('_GPT', '_MV')\n","    )\n","\n","    final_df = pd.DataFrame({\n","        'Original': merged_df['Original'],\n","        'Filtered GPT 4o': merged_df['Translated_GPT'],\n","        'Filtered Multi-VALUE': merged_df['Translated_MV']\n","    })\n","\n","    print(f\"\\nProcessed {dialect} - {dataset}\")\n","    print(f\"Original GPT rows: {len(gpt_df)}\")\n","    print(f\"Original MV rows: {len(mv_df)}\")\n","    print(f\"Final merged rows: {len(final_df)}\")\n","    print(f\"Final unique originals: {final_df['Original'].nunique()}\")\n","\n","    output_path = os.path.join(output_dir, f\"aligned_{dataset.lower().replace(' ', '_').replace('(', '').replace(')', '')}.csv\")\n","    final_df.to_csv(output_path, index=False)\n","\n","for dialect in tqdm(dialects, desc=\"Processing dialects\"):\n","    for dataset in tqdm(datasets, desc=f\"Processing datasets for {dialect}\"):\n","        try:\n","            process_dataset(dialect, dataset)\n","        except Exception as e:\n","            print(f\"Error processing {dialect} - {dataset}: {str(e)}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
