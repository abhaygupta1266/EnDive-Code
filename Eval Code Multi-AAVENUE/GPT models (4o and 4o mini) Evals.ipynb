{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y9q_F48kiVbO"
      },
      "outputs": [],
      "source": [
        "## NON-COT EVALS\n",
        "\n",
        "!pip install openai==0.28 pandas numpy regex tqdm\n",
        "\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from openai.error import RateLimitError, APIError\n",
        "\n",
        "openai.api_key = \"API-KEY\"\n",
        "TEMPERATURE = 0.7\n",
        "\n",
        "input_base_dir = \"/content/drive/MyDrive/!!Multi-AAVENUE/BLEU Score Filtered Datasets/GPT 4o\"\n",
        "output_base_dir = \"/content/drive/MyDrive/!!Multi-AAVENUE/Evaluation Results\"\n",
        "\n",
        "dialects = [\"IndE\", \"JamE\", \"AAVE\", \"CollSgE\", \"ChcE\"]\n",
        "\n",
        "datasets = {\n",
        "    \"SVAMP\": \"SVAMP(700)/SVAMP(700)_filtered_bleu_scores.csv\",\n",
        "    \"MBPP\": \"MBPP(374)/MBPP(374)_filtered_bleu_scores.csv\",\n",
        "    \"LogicBenchYN\": \"Logic Bench YN(500)/Logic Bench YN(500)_filtered_bleu_scores.json\",\n",
        "    \"LogicBenchMCQ\": \"Logic Bench MCQ(480)/Logic Bench MCQ(480)_filtered_bleu_scores.json\",\n",
        "    \"HumanEVAL\": \"HumanEVAL(164)/HumanEVAL(164)_filtered_bleu_scores.csv\",\n",
        "    \"GSM8K\": \"GSM8K(1000)/GSM8K(1000)_filtered_bleu_scores.csv\",\n",
        "    \"FOLIO\": \"FOLIO(1000)/FOLIO(1000)_filtered_bleu_scores.csv\",\n",
        "    \"WSC\": \"GLUE + SuperGLUE/WSC (659)/WSC (659)_filtered_bleu_scores.csv\",\n",
        "    \"SST-2\": \"GLUE + SuperGLUE/SST-2 (1000)/SST-2 (1000)_filtered_bleu_scores.csv\",\n",
        "    \"MultiRC\": \"GLUE + SuperGLUE/MultiRC (1000)/MultiRC (1000)_filtered_bleu_scores.csv\",\n",
        "    \"COPA\": \"GLUE + SuperGLUE/COPA (500)/COPA (500)_filtered_bleu_scores.csv\",\n",
        "    \"BoolQ\": \"GLUE + SuperGLUE/BoolQ (1000)/BoolQ (1000)_filtered_bleu_scores.csv\"\n",
        "}\n",
        "\n",
        "models = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
        "\n",
        "def write_row_to_csv(row: dict, filename: str):\n",
        "    mode = 'a'\n",
        "    with open(filename, mode, newline='', encoding=\"utf-8\") as csvfile:\n",
        "        df = pd.DataFrame([row])\n",
        "        write_header = csvfile.tell() == 0\n",
        "        df.to_csv(csvfile, index=False, header=write_header)\n",
        "        csvfile.flush()\n",
        "\n",
        "def prompt_gpt_model(model_name: str, system_message: str, user_message: str, retries=5, backoff_factor=2) -> str:\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_message},\n",
        "                    {\"role\": \"user\", \"content\": user_message}\n",
        "                ],\n",
        "                temperature=TEMPERATURE\n",
        "            )\n",
        "            return response['choices'][0]['message']['content']\n",
        "        except (RateLimitError, APIError) as e:\n",
        "            wait_time = backoff_factor ** attempt\n",
        "            print(f\"API error: {e}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    raise Exception(\"Maximum retries exceeded.\")\n",
        "\n",
        "def clean_code(generated_code: str) -> str:\n",
        "    cleaned_code = re.sub(r\"```(?:python)?\", \"\", generated_code, flags=re.DOTALL)\n",
        "    cleaned_code = re.sub(r\"```\", \"\", cleaned_code, flags=re.DOTALL)\n",
        "    return cleaned_code.strip()\n",
        "\n",
        "def extract_response(response: str, pattern: str) -> str:\n",
        "    match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)\n",
        "    return match.group(1).strip() if match else \"\"\n",
        "\n",
        "def evaluate_response(model_answer: str, expected_answer: str) -> bool:\n",
        "    return model_answer.strip().lower() == expected_answer.strip().lower()\n",
        "\n",
        "def run_test_cases(generated_code: str, test_cases: str) -> (bool, str):\n",
        "    try:\n",
        "        exec_globals = {}\n",
        "        exec(generated_code, exec_globals)\n",
        "        exec(test_cases, exec_globals)\n",
        "        return True, \"\"\n",
        "    except AssertionError as e:\n",
        "        return False, f\"AssertionError: {str(e)}\"\n",
        "    except SyntaxError as e:\n",
        "        return False, f\"SyntaxError: {str(e)}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"RuntimeError: {str(e)}\"\n",
        "\n",
        "model_output_mapping = {\n",
        "    \"gpt-4o\": os.path.join(output_base_dir, \"GPT4o_NonCoT\"),\n",
        "    \"gpt-4o-mini\": os.path.join(output_base_dir, \"GPT4oMini_NonCoT\")\n",
        "}\n",
        "\n",
        "def process_dataset(model_name: str, dataset_name: str, file_path: str, dialect: str):\n",
        "    if dataset_name in [\"LogicBenchYN\", \"LogicBenchMCQ\"]:\n",
        "        with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "    else:\n",
        "        data = pd.read_csv(file_path)\n",
        "    if dataset_name not in [\"LogicBenchYN\", \"LogicBenchMCQ\"]:\n",
        "        data = data.drop(columns=[col for col in data.columns if \"BLEU Score\" in col], errors=\"ignore\")\n",
        "    if model_name in model_output_mapping:\n",
        "        model_output_dir = os.path.join(model_output_mapping[model_name], dialect, dataset_name)\n",
        "    else:\n",
        "        model_output_dir = os.path.join(output_base_dir, model_name.replace(\"-\", \"\"), dialect, dataset_name)\n",
        "    os.makedirs(model_output_dir, exist_ok=True)\n",
        "    csv_file = os.path.join(model_output_dir, f\"{dataset_name}_results.csv\")\n",
        "    if dataset_name == \"FOLIO\" and model_name == \"gpt-4o\" and dialect == \"AAVE\" and os.path.exists(csv_file):\n",
        "        try:\n",
        "            processed = pd.read_csv(csv_file)\n",
        "            offset = len(processed)\n",
        "            print(f\"Resuming FOLIO processing for {dialect} ({model_name}): skipping first {offset} datapoints.\")\n",
        "            if isinstance(data, pd.DataFrame):\n",
        "                data = data.iloc[offset:]\n",
        "            else:\n",
        "                data = data[offset:]\n",
        "        except Exception as e:\n",
        "            print(f\"Could not determine processed count for FOLIO: {e}\")\n",
        "    correct_count = 0\n",
        "    total = 0\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        total_rows = len(data)\n",
        "    else:\n",
        "        total_rows = len(data)\n",
        "    if dataset_name == \"SVAMP\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            problem = row[f\"{dialect} (Original)\"]\n",
        "            question = row[\"Question\"]\n",
        "            expected = str(row[\"Answer\"])\n",
        "            user_prompt = (\n",
        "                \"Given a math word problem, provide the final numeric answer.\\n\"\n",
        "                f\"Context: {problem}\\nQuestion: {question}\\n\"\n",
        "                \"Answer:\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(.+)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Original)\": problem,\n",
        "                \"Question\": question,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"MBPP\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            problem = row[f\"{dialect} (Original)\"]\n",
        "            test_cases = row[\"Test_Cases\"]\n",
        "            user_prompt = (\n",
        "                \"Given a coding problem, produce a Python function.\\n\"\n",
        "                \"Start it with 'Answer:' on its own line.\\n\"\n",
        "                f\"Problem: {problem}\\nTest Cases: {test_cases}\\nAnswer:\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            generated_code = clean_code(response)\n",
        "            pattern = r\"Answer:\\s*(.+)\"\n",
        "            code = extract_response(generated_code, pattern)\n",
        "            is_correct, error_msg = run_test_cases(code, test_cases)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Original)\": problem,\n",
        "                \"Code\": code,\n",
        "                \"Correct\": int(is_correct),\n",
        "                \"Error Message\": error_msg\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"LogicBenchYN\":\n",
        "        pbar = tqdm(data, total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for task in pbar:\n",
        "            context = task[f\"{dialect} (context)\"]\n",
        "            for i in range(1, 5):\n",
        "                question_key = f\"Question {i}\"\n",
        "                answer_key = f\"Answer {i}\"\n",
        "                question = task.get(question_key, \"\")\n",
        "                expected = task.get(answer_key, \"\")\n",
        "                if not question or not expected:\n",
        "                    continue\n",
        "                user_prompt = (\n",
        "                    \"Given this context, answer yes or no.\\n\"\n",
        "                    f\"Context: {context}\\nQuestion: {question}\\n\"\n",
        "                    \"Answer:\"\n",
        "                )\n",
        "                system_prompt = \"You are a helpful assistant.\"\n",
        "                response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "                pattern = r\"(yes|no)\"\n",
        "                match = re.search(pattern, response, re.IGNORECASE)\n",
        "                if match:\n",
        "                    model_answer = match.group(1).lower()\n",
        "                else:\n",
        "                    model_answer = \"\"\n",
        "                is_correct = evaluate_response(model_answer, expected)\n",
        "                total += 1\n",
        "                if is_correct:\n",
        "                    correct_count += 1\n",
        "                row_dict = {\n",
        "                    f\"{dialect} (Context)\": context,\n",
        "                    \"Question\": question,\n",
        "                    \"Expected Answer\": expected,\n",
        "                    \"Model Answer\": model_answer,\n",
        "                    \"Correct\": is_correct\n",
        "                }\n",
        "                write_row_to_csv(row_dict, csv_file)\n",
        "                if total % 2 == 0:\n",
        "                    acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                    pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"LogicBenchMCQ\":\n",
        "        pbar = tqdm(data, total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for task in pbar:\n",
        "            context = task[f\"{dialect} (context)\"]\n",
        "            choices = [task.get(f\"Choice {i+1}\", \"\") for i in range(4)]\n",
        "            expected = task.get(\"Answer\", \"\")\n",
        "            if not all(choices) or not expected:\n",
        "                continue\n",
        "            user_prompt = (\n",
        "                \"Select the correct choice from 1, 2, 3, or 4.\\n\"\n",
        "                f\"Context: {context}\\nChoice 1: {choices[0]}\\nChoice 2: {choices[1]}\\n\"\n",
        "                f\"Choice 3: {choices[2]}\\nChoice 4: {choices[3]}\\n\"\n",
        "                \"Answer:\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(choice_\\d|\\d)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = (model_answer.lower() == expected.lower())\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Context)\": context,\n",
        "                \"Choice 1\": choices[0],\n",
        "                \"Choice 2\": choices[1],\n",
        "                \"Choice 3\": choices[2],\n",
        "                \"Choice 4\": choices[3],\n",
        "                \"Expected Answer\": expected,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"})\n",
        "        pbar.close()\n",
        "        df_processed = pd.read_csv(csv_file)\n",
        "        df_processed.to_csv(csv_file, index=False)\n",
        "    elif dataset_name == \"HumanEVAL\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            prompt_text = row[f\"{dialect} (Prompt)\"]\n",
        "            test_cases = row[\"Test_Cases\"]\n",
        "            user_prompt = (\n",
        "                \"Produce Python code. Start with 'Answer:'.\\n\"\n",
        "                f\"Problem: {prompt_text}\\nTest Cases: {test_cases}\\n\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            generated_code = clean_code(response)\n",
        "            pattern = r\"Answer:\\s*(.+)\"\n",
        "            code = extract_response(generated_code, pattern)\n",
        "            is_correct, error_msg = run_test_cases(code, test_cases)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Prompt)\": prompt_text,\n",
        "                \"Code\": code,\n",
        "                \"Correct\": int(is_correct),\n",
        "                \"Error Message\": error_msg\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"GSM8K\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            problem = row[f\"{dialect} (Original)\"]\n",
        "            expected = str(row[\"Answer\"])\n",
        "            user_prompt = (\n",
        "                \"Given the math problem:\\n\"\n",
        "                f\"{problem}\\n\"\n",
        "                \"Provide your final numeric answer.\\n\"\n",
        "                \"Answer:\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*([0-9\\.\\-]+)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Original)\": problem,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"FOLIO\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            premises = row[f\"{dialect} (Premises)\"]\n",
        "            conclusion = row[\"Conclusion\"]\n",
        "            expected = row[\"Label\"]\n",
        "            user_prompt = (\n",
        "                f\"Determine if the conclusion follows from the premises.\\n\"\n",
        "                f\"Premises: {premises}\\nConclusion: {conclusion}\\n\"\n",
        "                \"Answer: True, False, or Uncertain.\\n\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(True|False|Uncertain)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Premises)\": premises,\n",
        "                \"Conclusion\": conclusion,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"WSC\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            paragraph = row[f\"{dialect} (Original Paragraph)\"]\n",
        "            span1 = row[\"Span 1\"]\n",
        "            span2 = row[\"Span 2\"]\n",
        "            expected = str(row[\"Actual Label\"])\n",
        "            user_prompt = (\n",
        "                f\"Check if Span 2 refers to Span 1 in the paragraph.\\n\"\n",
        "                f\"Paragraph: {paragraph}\\nSpan 1: {span1}\\nSpan 2: {span2}\\n\"\n",
        "                \"Answer (1 if same, 0 if not):\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(\\d)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Original Paragraph)\": paragraph,\n",
        "                \"Span 1\": span1,\n",
        "                \"Span 2\": span2,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"SST-2\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            sentence = row[f\"{dialect} (Original Sentence)\"]\n",
        "            expected = str(row[\"Actual Label\"])\n",
        "            user_prompt = (\n",
        "                f\"Is the sentiment of this sentence positive (1) or negative (0)?\\n\"\n",
        "                f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
        "                \"Answer:\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(\\d)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Original Sentence)\": sentence,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"MultiRC\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            paragraph = row[f\"{dialect} (Paragraph)\"]\n",
        "            question = row[\"Question\"]\n",
        "            answer_choice = row[\"Answer Choice\"]\n",
        "            expected = str(row[\"Actual Label\"])\n",
        "            user_prompt = (\n",
        "                f\"Given a paragraph, a question, and an answer choice, is the choice correct (1) or incorrect (0)?\\n\"\n",
        "                f\"Paragraph: {paragraph}\\nQuestion: {question}\\nAnswer Choice: {answer_choice}\\n\"\n",
        "                \"Answer:\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(\\d)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Paragraph)\": paragraph,\n",
        "                \"Question\": question,\n",
        "                \"Answer Choice\": answer_choice,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"} )\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"COPA\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            premise = row[f\"{dialect} (Premise)\"]\n",
        "            choice1 = row[\"Choice 1\"]\n",
        "            choice2 = row[\"Choice 2\"]\n",
        "            expected = str(row[\"Actual Answer\"])\n",
        "            user_prompt = (\n",
        "                f\"Given a premise and two choices, pick which is more plausible (0 or 1).\\n\"\n",
        "                f\"Premise: {premise}\\n\"\n",
        "                f\"Choice 1: {choice1}\\n\"\n",
        "                f\"Choice 2: {choice2}\\n\"\n",
        "                \"Answer:\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(\\d)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Premise)\": premise,\n",
        "                \"Choice 1\": choice1,\n",
        "                \"Choice 2\": choice2,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"} )\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"BoolQ\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            passage = row[f\"{dialect} (SAE Passage)\"]\n",
        "            question = row[\"SAE Question\"]\n",
        "            expected = str(row[\"Actual Label\"])\n",
        "            user_prompt = (\n",
        "                f\"Passage: \\\"{passage}\\\"\\n\"\n",
        "                f\"Question: \\\"{question}\\\"\\n\"\n",
        "                \"Is the answer TRUE or FALSE?\\nAnswer:\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(TRUE|FALSE)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (SAE Passage)\": passage,\n",
        "                \"SAE Question\": question,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"} )\n",
        "        pbar.close()\n",
        "    else:\n",
        "        print(f\"Dataset {dataset_name} not recognized for processing.\")\n",
        "        return\n",
        "    accuracy = (correct_count / total * 100) if total > 0 else 0\n",
        "    with open(os.path.join(model_output_dir, f\"{dataset_name}_accuracy.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"Total instances: {total}\\n\")\n",
        "        f.write(f\"Correct answers: {correct_count}\\n\")\n",
        "        f.write(f\"Accuracy: {accuracy:.2f}%\\n\")\n",
        "\n",
        "for model in models:\n",
        "    for dialect in dialects:\n",
        "        for dataset_name, rel_path in datasets.items():\n",
        "            full_path = os.path.join(input_base_dir, dialect, rel_path)\n",
        "            if os.path.exists(full_path):\n",
        "                process_dataset(model, dataset_name, full_path, dialect)\n",
        "            else:\n",
        "                print(f\"File not found: {full_path}\")\n",
        "\n",
        "\n",
        "print(\"Non-CoT evaluation complete! Results have been saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## COT Evals\n",
        "\n",
        "!pip install openai==0.28 pandas numpy regex tqdm\n",
        "\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from openai.error import RateLimitError, APIError\n",
        "\n",
        "openai.api_key = \"API-KEY\"\n",
        "TEMPERATURE = 0.7\n",
        "\n",
        "input_base_dir = \"/content/drive/MyDrive/!!Multi-AAVENUE/BLEU Score Filtered Datasets/GPT 4o\"\n",
        "output_base_dir = \"/content/drive/MyDrive/!!Multi-AAVENUE/Evaluation Results\"\n",
        "\n",
        "dialects = [\"IndE\", \"JamE\", \"AAVE\", \"CollSgE\", \"ChcE\"]\n",
        "\n",
        "datasets = {\n",
        "    \"SVAMP\": \"SVAMP(700)/SVAMP(700)_filtered_bleu_scores.csv\",\n",
        "    \"MBPP\": \"MBPP(374)/MBPP(374)_filtered_bleu_scores.csv\",\n",
        "    \"LogicBenchYN\": \"Logic Bench YN(500)/Logic Bench YN(500)_filtered_bleu_scores.json\",\n",
        "    \"LogicBenchMCQ\": \"Logic Bench MCQ(480)/Logic Bench MCQ(480)_filtered_bleu_scores.json\",\n",
        "    \"HumanEVAL\": \"HumanEVAL(164)/HumanEVAL(164)_filtered_bleu_scores.csv\",\n",
        "    \"GSM8K\": \"GSM8K(1000)/GSM8K(1000)_filtered_bleu_scores.csv\",\n",
        "    \"FOLIO\": \"FOLIO(1000)/FOLIO(1000)_filtered_bleu_scores.csv\",\n",
        "    \"WSC\": \"GLUE + SuperGLUE/WSC (659)/WSC (659)_filtered_bleu_scores.csv\",\n",
        "    \"SST-2\": \"GLUE + SuperGLUE/SST-2 (1000)/SST-2 (1000)_filtered_bleu_scores.csv\",\n",
        "    \"MultiRC\": \"GLUE + SuperGLUE/MultiRC (1000)/MultiRC (1000)_filtered_bleu_scores.csv\",\n",
        "    \"COPA\": \"GLUE + SuperGLUE/COPA (500)/COPA (500)_filtered_bleu_scores.csv\",\n",
        "    \"BoolQ\": \"GLUE + SuperGLUE/BoolQ (1000)/BoolQ (1000)_filtered_bleu_scores.csv\"\n",
        "}\n",
        "\n",
        "models = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
        "\n",
        "def write_row_to_csv(row: dict, filename: str):\n",
        "    mode = 'a'\n",
        "    with open(filename, mode, newline='', encoding=\"utf-8\") as csvfile:\n",
        "        df = pd.DataFrame([row])\n",
        "        write_header = csvfile.tell() == 0\n",
        "        df.to_csv(csvfile, index=False, header=write_header)\n",
        "        csvfile.flush()\n",
        "\n",
        "def prompt_gpt_model(model_name: str, system_message: str, user_message: str, retries=5, backoff_factor=2) -> str:\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_message},\n",
        "                    {\"role\": \"user\", \"content\": user_message}\n",
        "                ],\n",
        "                temperature=TEMPERATURE\n",
        "            )\n",
        "            return response['choices'][0]['message']['content']\n",
        "        except (RateLimitError, APIError) as e:\n",
        "            wait_time = backoff_factor ** attempt\n",
        "            print(f\"API error: {e}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "    raise Exception(\"Maximum retries exceeded.\")\n",
        "\n",
        "def clean_code(generated_code: str) -> str:\n",
        "    cleaned_code = re.sub(r\"```(?:python)?\", \"\", generated_code, flags=re.DOTALL)\n",
        "    cleaned_code = re.sub(r\"```\", \"\", cleaned_code, flags=re.DOTALL)\n",
        "    return cleaned_code.strip()\n",
        "\n",
        "def extract_response(response: str, pattern: str) -> str:\n",
        "    match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)\n",
        "    return match.group(1).strip() if match else \"\"\n",
        "\n",
        "def evaluate_response(model_answer: str, expected_answer: str) -> bool:\n",
        "    return model_answer.strip().lower() == expected_answer.strip().lower()\n",
        "\n",
        "def run_test_cases(generated_code: str, test_cases: str) -> (bool, str):\n",
        "    try:\n",
        "        exec_globals = {}\n",
        "        exec(generated_code, exec_globals)\n",
        "        exec(test_cases, exec_globals)\n",
        "        return True, \"\"\n",
        "    except AssertionError as e:\n",
        "        return False, f\"AssertionError: {str(e)}\"\n",
        "    except SyntaxError as e:\n",
        "        return False, f\"SyntaxError: {str(e)}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"RuntimeError: {str(e)}\"\n",
        "\n",
        "model_output_mapping = {\n",
        "    \"gpt-4o\": os.path.join(output_base_dir, \"GPT4o_CoT\"),\n",
        "    \"gpt-4o-mini\": os.path.join(output_base_dir, \"GPT4oMini_CoT\")\n",
        "}\n",
        "\n",
        "def process_dataset(model_name: str, dataset_name: str, file_path: str, dialect: str):\n",
        "    if dataset_name in [\"LogicBenchYN\", \"LogicBenchMCQ\"]:\n",
        "        with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "    else:\n",
        "        data = pd.read_csv(file_path)\n",
        "    if dataset_name not in [\"LogicBenchYN\", \"LogicBenchMCQ\"]:\n",
        "        data = data.drop(columns=[col for col in data.columns if \"BLEU Score\" in col], errors=\"ignore\")\n",
        "    if model_name in model_output_mapping:\n",
        "        model_output_dir = os.path.join(model_output_mapping[model_name], dialect, dataset_name)\n",
        "    else:\n",
        "        model_output_dir = os.path.join(output_base_dir, model_name.replace(\"-\", \"\"), dialect, dataset_name)\n",
        "    os.makedirs(model_output_dir, exist_ok=True)\n",
        "    csv_file = os.path.join(model_output_dir, f\"{dataset_name}_results.csv\")\n",
        "    if dataset_name == \"FOLIO\" and model_name == \"gpt-4o\" and dialect == \"AAVE\" and os.path.exists(csv_file):\n",
        "        try:\n",
        "            processed = pd.read_csv(csv_file)\n",
        "            offset = len(processed)\n",
        "            print(f\"Resuming FOLIO processing for {dialect} ({model_name}): skipping first {offset} datapoints.\")\n",
        "            if isinstance(data, pd.DataFrame):\n",
        "                data = data.iloc[offset:]\n",
        "            else:\n",
        "                data = data[offset:]\n",
        "        except Exception as e:\n",
        "            print(f\"Could not determine processed count for FOLIO: {e}\")\n",
        "    correct_count = 0\n",
        "    total = 0\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        total_rows = len(data)\n",
        "    else:\n",
        "        total_rows = len(data)\n",
        "    if dataset_name == \"SVAMP\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            problem = row[f\"{dialect} (Original)\"]\n",
        "            question = row[\"Question\"]\n",
        "            expected = str(row[\"Answer\"])\n",
        "            user_prompt = (\n",
        "                \"Let's think carefully about the math word problem step by step.\\n\"\n",
        "                f\"Context: {problem}\\nQuestion: {question}\\n\"\n",
        "                \"Finally, provide your numeric answer in the format: Answer: <number>\\n\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(.+)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Original)\": problem,\n",
        "                \"Question\": question,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"COT Response\": response,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"MBPP\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            problem = row[f\"{dialect} (Original)\"]\n",
        "            test_cases = row[\"Test_Cases\"]\n",
        "            user_prompt = (\n",
        "                \"Let's break down the coding problem step by step. Then write a Python function.\\n\"\n",
        "                \"Start with 'Answer:' and no markdown.\\n\"\n",
        "                f\"Problem: {problem}\\nTest Cases: {test_cases}\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            generated_code = clean_code(response)\n",
        "            pattern = r\"Answer:\\s*(.+)\"\n",
        "            code = extract_response(generated_code, pattern)\n",
        "            is_correct, error_msg = run_test_cases(code, test_cases)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Original)\": problem,\n",
        "                \"Code\": code,\n",
        "                \"COT Response\": response,\n",
        "                \"Correct\": int(is_correct),\n",
        "                \"Error Message\": error_msg\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"LogicBenchYN\":\n",
        "        pbar = tqdm(data, total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for task in pbar:\n",
        "            context = task[f\"{dialect} (context)\"]\n",
        "            for i in range(1, 5):\n",
        "                question_key = f\"Question {i}\"\n",
        "                answer_key = f\"Answer {i}\"\n",
        "                question = task.get(question_key, \"\")\n",
        "                expected = task.get(answer_key, \"\")\n",
        "                if not question or not expected:\n",
        "                    continue\n",
        "                user_prompt = (\n",
        "                    \"Let's reason about the question step by step.\\n\"\n",
        "                    f\"Context: {context}\\nQuestion: {question}\\n\"\n",
        "                    \"Finally, respond EXACTLY as Answer: yes or Answer: no.\"\n",
        "                )\n",
        "                system_prompt = \"You are a helpful assistant.\"\n",
        "                response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "                pattern = r\"Answer:\\s*(yes|no)\"\n",
        "                model_answer = extract_response(response, pattern)\n",
        "                is_correct = evaluate_response(model_answer, expected)\n",
        "                total += 1\n",
        "                if is_correct:\n",
        "                    correct_count += 1\n",
        "                row_dict = {\n",
        "                    f\"{dialect} (Context)\": context,\n",
        "                    \"Question\": question,\n",
        "                    \"Expected Answer\": expected,\n",
        "                    \"COT Response\": response,\n",
        "                    \"Model Answer\": model_answer,\n",
        "                    \"Correct\": is_correct\n",
        "                }\n",
        "                write_row_to_csv(row_dict, csv_file)\n",
        "                if total % 2 == 0:\n",
        "                    acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                    pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"LogicBenchMCQ\":\n",
        "        pbar = tqdm(data, total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for task in pbar:\n",
        "            context = task[f\"{dialect} (context)\"]\n",
        "            choices = [task.get(f\"Choice {i+1}\", \"\") for i in range(4)]\n",
        "            expected = task.get(\"Answer\", \"\")\n",
        "            if not all(choices) or not expected:\n",
        "                continue\n",
        "            user_prompt = (\n",
        "                \"Let's analyze the context and each choice step by step.\\n\"\n",
        "                \"Finally, provide EXACTLY one line like: Answer: choice_2.\\n\"\n",
        "                f\"Context: {context}\\n\"\n",
        "                f\"Choice 1: {choices[0]}\\nChoice 2: {choices[1]}\\nChoice 3: {choices[2]}\\nChoice 4: {choices[3]}\\n\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(choice_\\d)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Context)\": context,\n",
        "                \"Choice 1\": choices[0],\n",
        "                \"Choice 2\": choices[1],\n",
        "                \"Choice 3\": choices[2],\n",
        "                \"Choice 4\": choices[3],\n",
        "                \"Expected Answer\": expected,\n",
        "                \"COT Response\": response,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"} )\n",
        "        pbar.close()\n",
        "        df_processed = pd.read_csv(csv_file)\n",
        "        df_processed.to_csv(csv_file, index=False)\n",
        "    elif dataset_name == \"HumanEVAL\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            prompt_text = row[f\"{dialect} (Prompt)\"]\n",
        "            test_cases = row[\"Test_Cases\"]\n",
        "            user_prompt = (\n",
        "                \"Let's think step by step about the coding task.\\n\"\n",
        "                \"Finally, provide code beginning with 'Answer:'.\\n\\n\"\n",
        "                f\"Problem: {prompt_text}\\nTest Cases: {test_cases}\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            generated_code = clean_code(response)\n",
        "            pattern = r\"Answer:\\s*(.+)\"\n",
        "            code = extract_response(generated_code, pattern)\n",
        "            is_correct, error_msg = run_test_cases(code, test_cases)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Prompt)\": prompt_text,\n",
        "                \"Code\": code,\n",
        "                \"COT Response\": response,\n",
        "                \"Correct\": int(is_correct),\n",
        "                \"Error Message\": error_msg\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc:.2f}%\"})\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"GSM8K\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            problem = row[f\"{dialect} (Original)\"]\n",
        "            expected = str(row[\"Answer\"])\n",
        "            user_prompt = (\n",
        "                \"Let's think step by step to solve this math problem.\\n\"\n",
        "                \"Finally, provide a numeric result as: Answer: <number>.\\n\\n\"\n",
        "                f\"Problem: {problem}\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*([0-9.\\-]+)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Original)\": problem,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"COT Response\": response,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"} )\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"FOLIO\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            premises = row[f\"{dialect} (Premises)\"]\n",
        "            conclusion = row[\"Conclusion\"]\n",
        "            expected = row[\"Label\"]\n",
        "            user_prompt = (\n",
        "                \"Let's analyze whether the conclusion follows from the premises step by step.\\n\"\n",
        "                \"Finally, provide: Answer: True, False, or Uncertain.\\n\\n\"\n",
        "                f\"Premises: {premises}\\nConclusion: {conclusion}\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(True|False|Uncertain)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Premises)\": premises,\n",
        "                \"Conclusion\": conclusion,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"COT Response\": response,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"} )\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"WSC\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            paragraph = row[f\"{dialect} (Original Paragraph)\"]\n",
        "            span1 = row[\"Span 1\"]\n",
        "            span2 = row[\"Span 2\"]\n",
        "            expected = str(row[\"Actual Label\"])\n",
        "            user_prompt = (\n",
        "                \"Let's analyze the reference step by step.\\n\"\n",
        "                \"Finally, provide: Answer: 1 if Span 2 refers to Span 1, else 0.\\n\\n\"\n",
        "                f\"Paragraph: {paragraph}\\nSpan 1: {span1}\\nSpan 2: {span2}\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(\\d)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Original Paragraph)\": paragraph,\n",
        "                \"Span 1\": span1,\n",
        "                \"Span 2\": span2,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"COT Response\": response,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"} )\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"SST-2\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            sentence = row[f\"{dialect} (Original Sentence)\"]\n",
        "            expected = str(row[\"Actual Label\"])\n",
        "            user_prompt = (\n",
        "                \"Let's assess the sentiment step by step.\\n\"\n",
        "                \"Finally, provide: Answer: 1 if positive, 0 if negative.\\n\\n\"\n",
        "                f\"Sentence: \\\"{sentence}\\\"\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(\\d)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Original Sentence)\": sentence,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"COT Response\": response,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"} )\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"MultiRC\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            paragraph = row[f\"{dialect} (Paragraph)\"]\n",
        "            question = row[\"Question\"]\n",
        "            answer_choice = row[\"Answer Choice\"]\n",
        "            expected = str(row[\"Actual Label\"])\n",
        "            user_prompt = (\n",
        "                \"Let's analyze the paragraph and question step by step.\\n\"\n",
        "                \"Finally, provide: Answer: 1 if correct, 0 if incorrect.\\n\\n\"\n",
        "                f\"Paragraph: {paragraph}\\nQuestion: {question}\\nAnswer Choice: {answer_choice}\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(\\d)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Paragraph)\": paragraph,\n",
        "                \"Question\": question,\n",
        "                \"Answer Choice\": answer_choice,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"COT Response\": response,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"} )\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"COPA\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            premise = row[f\"{dialect} (Premise)\"]\n",
        "            choice1 = row[\"Choice 1\"]\n",
        "            choice2 = row[\"Choice 2\"]\n",
        "            expected = str(row[\"Actual Answer\"])\n",
        "            user_prompt = (\n",
        "                \"Let's compare the plausibility of these two choices step by step.\\n\"\n",
        "                \"Finally, provide: Answer: 0 for the first, or 1 for the second.\\n\\n\"\n",
        "                f\"Premise: {premise}\\nChoice 1: {choice1}\\nChoice 2: {choice2}\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(\\d)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (Premise)\": premise,\n",
        "                \"Choice 1\": choice1,\n",
        "                \"Choice 2\": choice2,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"COT Response\": response,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"} )\n",
        "        pbar.close()\n",
        "    elif dataset_name == \"BoolQ\":\n",
        "        pbar = tqdm(data.iterrows(), total=total_rows, desc=f\"{model_name} | {dataset_name} | {dialect}\", unit=\"row\")\n",
        "        for idx, row in pbar:\n",
        "            passage = row[f\"{dialect} (SAE Passage)\"]\n",
        "            question = row[\"SAE Question\"]\n",
        "            expected = str(row[\"Actual Label\"])\n",
        "            user_prompt = (\n",
        "                \"Let's review the passage and question step by step.\\n\"\n",
        "                \"Finally, provide: Answer: TRUE or FALSE.\\n\\n\"\n",
        "                f\"Passage: \\\"{passage}\\\"\\nQuestion: \\\"{question}\\\"\"\n",
        "            )\n",
        "            system_prompt = \"You are a helpful assistant.\"\n",
        "            response = prompt_gpt_model(model_name, system_prompt, user_prompt)\n",
        "            pattern = r\"Answer:\\s*(TRUE|FALSE)\"\n",
        "            model_answer = extract_response(response, pattern)\n",
        "            is_correct = evaluate_response(model_answer, expected)\n",
        "            total += 1\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            row_dict = {\n",
        "                f\"{dialect} (SAE Passage)\": passage,\n",
        "                \"SAE Question\": question,\n",
        "                \"Expected Answer\": expected,\n",
        "                \"COT Response\": response,\n",
        "                \"Model Answer\": model_answer,\n",
        "                \"Correct\": is_correct\n",
        "            }\n",
        "            write_row_to_csv(row_dict, csv_file)\n",
        "            if total % 2 == 0:\n",
        "                acc_percentage = (correct_count / total) * 100 if total > 0 else 0\n",
        "                pbar.set_postfix({\"Acc\": f\"{acc_percentage:.2f}%\"} )\n",
        "        pbar.close()\n",
        "    else:\n",
        "        print(f\"Dataset {dataset_name} not recognized for processing.\")\n",
        "        return\n",
        "    accuracy = (correct_count / total * 100) if total > 0 else 0\n",
        "    with open(os.path.join(model_output_dir, f\"{dataset_name}_accuracy.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"Total instances: {total}\\n\")\n",
        "        f.write(f\"Correct answers: {correct_count}\\n\")\n",
        "        f.write(f\"Accuracy: {accuracy:.2f}%\\n\")\n",
        "\n",
        "for model in models:\n",
        "    for dialect in dialects:\n",
        "        for dataset_name, rel_path in datasets.items():\n",
        "            full_path = os.path.join(input_base_dir, dialect, rel_path)\n",
        "            if os.path.exists(full_path):\n",
        "                process_dataset(model, dataset_name, full_path, dialect)\n",
        "            else:\n",
        "                print(f\"File not found: {full_path}\")\n",
        "\n",
        "\n",
        "print(\"CoT evaluation complete! Results have been saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
